# An unique identifier for the head node and workers of this cluster.
cluster_name: imagenet

# The maximum number of workers nodes to launch in addition to the head
# node. This takes precedence over min_workers. min_workers default to 0.
min_workers: 3
max_workers: 3

# docker:
#     image: tensorflow/tensorflow:1.5.0-py3
#     container_name: ray_docker

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-east-1
    cache_stopped_nodes: False
    # availability_zone: us-east-1a

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu

head_node:
    InstanceType: p3.16xlarge
    ImageId: ami-0d96d570269578cd7
    BlockDeviceMappings:
      - DeviceName: "/dev/sdm"
        Ebs:
          VolumeType: "io1"
          Iops: 10000
          DeleteOnTermination: True
          VolumeSize: 250
          SnapshotId: "snap-01838dca0cbffad5c"
      - DeviceName: /dev/sda1
        Ebs:
            VolumeSize: 200
            DeleteOnTermination: True

# worker_nodes:
#     InstanceType: c5.9xlarge
#     ImageId: ami-0d96d570269578cd7
#     InstanceMarketOptions:
#         MarketType: spot
#         SpotOptions:
#             MaxPrice: "1.0"
#     BlockDeviceMappings:
#         - DeviceName: /dev/sda1
#           Ebs:
#               VolumeSize: 150

worker_nodes:
    InstanceType: p3.16xlarge
    ImageId: ami-0d96d570269578cd7
    InstanceMarketOptions:
        MarketType: spot
    BlockDeviceMappings:
      - DeviceName: "/dev/sdm"
        Ebs:
          VolumeType: "io1"
          Iops: 10000
          DeleteOnTermination: True
          VolumeSize: 250
          SnapshotId: "snap-01838dca0cbffad5c"
      - DeviceName: /dev/sda1
        Ebs:
            VolumeSize: 200
            DeleteOnTermination: True

setup_commands:
    - sudo mkdir /data || true
    - sudo mount /dev/xvdm /data || true
    - pip install -U ray==0.7.6
    - pip install torch torchvision filelock ray[rllib] tensorboard pandas ipdb #tensorflow-gpu==1.12.0 ipdb ray[rllib]
    - >
        cd hypersched &&
        pip install -e ./


file_mounts: {
    "~/hypersched/": riselab/hypersched/
}

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []
